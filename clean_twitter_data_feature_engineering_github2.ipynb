{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing data_set by removing urls, @ and RTs from the tweet text:\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "#Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import tokenize\n",
    "\n",
    "#matplotlib.style.use('ggplot')\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing URLS and @ and twitter handles \n",
    "data_set['text'] = data_set['text'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))    \n",
    "data_set['text'] = data_set['text'].apply(lambda x: re.sub(r\"@\\S+\", \"\", x))      \n",
    "#Delete certain punctuations, put the text in lower case and delete the double space with the function apply.\n",
    "data_set['text'] = data_set['text'].apply(lambda x: re.sub('[!@#$:).;,?&]', '', x.lower()))\n",
    "data_set['text'] = data_set['text'].apply(lambda x: re.sub('  ', ' ', x))\n",
    "#removing the letter rt that don't have anything in front or after them\n",
    "data_set['text'] = data_set['text'].apply(lambda x: re.sub(r'(?<![a-z])rt(?![a-z])', '', x, flags=re.I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment analysis\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('vader_lexicon')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Sentiment analysis\n",
    "data_set['text_lem'] = [''.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in data_set['text']]       \n",
    "#Here I utilized both TFIDF and removed stop words. Note that I did not do that for the machine learning part.\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000, min_df=10, stop_words='english', use_idf=True)\n",
    "X = vectorizer.fit_transform(data_set['text_lem'].str.upper())\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "data_set['sentiment_compound_polarity']=data_set.text_lem.apply(lambda x:sid.polarity_scores(x)['compound'])\n",
    "data_set['sentiment_neutral']=data_set.text_lem.apply(lambda x:sid.polarity_scores(x)['neu'])\n",
    "data_set['sentiment_negative']=data_set.text_lem.apply(lambda x:sid.polarity_scores(x)['neg'])\n",
    "data_set['sentiment_pos']=data_set.text_lem.apply(lambda x:sid.polarity_scores(x)['pos'])\n",
    "data_set['sentiment_type']=''\n",
    "data_set.loc[data_set.sentiment_compound_polarity>0,'sentiment_type']='POSITIVE'\n",
    "data_set.loc[data_set.sentiment_compound_polarity==0,'sentiment_type']='NEUTRAL'\n",
    "data_set.loc[data_set.sentiment_compound_polarity<0,'sentiment_type']='NEGATIVE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment analysis graph for depressed tweets only\n",
    "#subsetting by throwing out not-depressed\n",
    "data_set_sentiment_depressed = data_set.loc[:,['depressed','sentiment_neutral', 'sentiment_type', 'text', 'text_lem']]\n",
    "data_set_sentiment_depressed = data_set_sentiment_depressed[data_set_sentiment_depressed.depressed != 0]\n",
    "\n",
    "data_set_sentiment = data_set_sentiment_depressed.groupby(['sentiment_type'])['sentiment_neutral'].count()\n",
    "data_set_sentiment.rename(\"\",inplace=True)\n",
    "explode = (1, 0, 0)\n",
    "plt.subplot(221)\n",
    "data_set_sentiment.transpose().plot(kind='barh',figsize=(20, 20))\n",
    "plt.title('Sentiment of Tweets by Depressed People', bbox={'facecolor':'0.8', 'pad':0})\n",
    "plt.subplot(222)\n",
    "data_set_sentiment.plot(kind='pie',figsize=(20, 20),autopct='%1.1f%%',shadow=True,explode=explode)\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=3, borderaxespad=0.)\n",
    "plt.title('Sentiment of Tweets by Depressed People', bbox={'facecolor':'0.8', 'pad':0})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment analysis graph for not-depressed tweets only\n",
    "#subsetting by throwing out depressed\n",
    "data_set_sentiment_not_depressed = data_set.loc[:,['depressed','sentiment_neutral', 'sentiment_type', 'text', 'text_lem']]\n",
    "data_set_sentiment_not_depressed = data_set_sentiment_not_depressed[data_set_sentiment_not_depressed.depressed != 1]\n",
    "\n",
    "data_set_sentiment = data_set_sentiment_not_depressed.groupby(['sentiment_type'])['sentiment_neutral'].count()\n",
    "data_set_sentiment.rename(\"\",inplace=True)\n",
    "explode = (1, 0, 0)\n",
    "plt.subplot(221)\n",
    "data_set_sentiment.transpose().plot(kind='barh',figsize=(20, 20))\n",
    "plt.title('Sentiment of Tweets by Not Depressed People', bbox={'facecolor':'0.8', 'pad':0})\n",
    "plt.subplot(222)\n",
    "data_set_sentiment.plot(kind='pie',figsize=(20, 20),autopct='%1.1f%%',shadow=True,explode=explode)\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc=3, borderaxespad=0.)\n",
    "plt.title('Sentiment of Tweets by Not Depressed People', bbox={'facecolor':'0.8', 'pad':0})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Cloud depressed Content Words\n",
    "#Deleting the punctuations, the URLs, putting the test in a lower case, extractting the username for examples\n",
    "def wordcloud(tweets,col):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wordcloud = WordCloud(background_color = \"white\", stopwords = stopwords, random_state = 2016).generate(\" \".join([i for i in tweets[col]]))\n",
    "    plt.figure( figsize = (10,5), facecolor = 'k')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"\")\n",
    "wordcloud(data_set_sentiment_depressed, 'text_lem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Cloud not depressed Content Words\n",
    "#Deleting the punctuations, the URLs, putting the test in a lower case, extractting the username for examples\n",
    "def wordcloud(tweets,col):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wordcloud = WordCloud(background_color = \"white\", stopwords = stopwords, random_state = 2016).generate(\" \".join([i for i in tweets[col]]))\n",
    "    plt.figure( figsize = (10,5), facecolor = 'k')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Good Morning Datascience+\")\n",
    "wordcloud(data_set_sentiment_not_depressed, 'text_lem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding to dataframe how many characters in tweet\n",
    "#did not end up using it, but here it is\n",
    "#data_set['text_len_char'] = data_set['text'].apply(len)\n",
    "\n",
    "#Based on previous reserach I hypothesized that depressed tweets would have a higher percentage of \"i\" relative to other words\n",
    "#Adding to dataframe percentage of 'i' in tweet\n",
    "#finding how many words in each tweet\n",
    "data_set['text_len_word'] = data_set['text'].apply(lambda x: re.findall(r'(\\S+)', x))\n",
    "data_set['text_len_word'] = data_set['text_len_word'].apply(len)\n",
    "#finding how many times \"i\" appears in each tweet by only keeping the free standing \"i\"s and then counting them\n",
    "data_set['text_i'] = data_set['text_lem'].apply(lambda x: re.findall(r'(?:\\s|^)i(?=\\s|$)', x))\n",
    "data_set['text_i'] = data_set['text_i'].apply(len)\n",
    "#Finding what percentage of \"i\" is of all tweet words\n",
    "data_set['text_i_percent'] = (data_set['text_i']/data_set['text_len_word'])*100\n",
    "\n",
    "#Examining the difference between depressed and non-depressed \"i\" usage:\n",
    "data_set.groupby(['depressed'])['text_i_percent'].mean()\n",
    "#Graphing it:\n",
    "import altair as alt\n",
    "\n",
    "# for the notebook only (not for JupyterLab) run this command once per session\n",
    "alt.renderers.enable('notebook')\n",
    "alt.Chart(data_set).mark_bar().encode(\n",
    "    x='depressed',\n",
    "    y='average(text_i_percent)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "data_set['text_i_percent'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you saved the timestamp data you can also examine whether the time of day depressed vs not-depressed tweet differs:\n",
    "data_set['created_at1'] = pd.to_datetime(data_set['created_at'])\n",
    "#keeping only the hour\n",
    "data_set['created_at1'] = data_set['created_at1'].dt.hour\n",
    "data_set['late_night'] = data_set['created_at1'].apply(lambda x: 'late night' if 8>x or x>21 else 'not late night')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save data as a pickle file:\n",
    "import jsonpickle\n",
    "import pandas as pd\n",
    "data_set = pd.read_pickle('/home/twitterdata.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save it as a CSV:\n",
    "data_set.to_csv('/home/twitterdata.csv', sep = ',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
