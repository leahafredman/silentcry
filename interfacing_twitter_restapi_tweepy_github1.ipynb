{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first step to any project is obtaining data. \n",
    "#Here, I interface with the Twitter REST API to download up to 3000 tweets of a specific individual by their twitter handle, and then discard anything beyond 3000.\n",
    "#What is nice about this code, is that if you want download enough tweets that the API would normally time out and stop working, this code ensures that it just take a pause, waits the necessary amount of time, and then continues.\n",
    "#it is also set up in a way that allows to continue downloading from the last place that you had to pause at, so that you can avoid duplicates\n",
    "\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 50\n",
    "pd.options.display.width = 150\n",
    "#\n",
    "#The keys are obtained with your twitter account through your twitter website\n",
    "#Authentication\n",
    "consumer_key = 'EnterYourConsumerKeyHere'\n",
    "consumer_secret = 'EnterYourConsumerSecretHere'\n",
    "#Passing the consumer key and secret to the OAuthHandler\n",
    "#auth = tweepy.OAuthHandler(consumer_key = consumer_key, consumer_secret = consumer_secret)\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "##Creating API object and passing the authentication to it\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "if (not api):\n",
    "    print (\"Can't Authenticate\")\n",
    "    sys.exit(-1)\n",
    "\n",
    "import sys\n",
    "import jsonpickle\n",
    "import os\n",
    "\n",
    "account_list1 = \"TwitterHandleOfInterestGoesHere\"\n",
    "#If you want ALL THE TWEETS enter some rediculously large number here\n",
    "maxTweets = 5000000000000000000000000000000000000\n",
    "#max tweets per query that api permits\n",
    "tweetsPerQry = 100\n",
    "\n",
    "\n",
    "results1 = []\n",
    "#if results1 from a specific ID onwards required set since_id to the ID otherwise default to no lower limit to go back as far back as API allows\n",
    "sinceId = None\n",
    "#if results1 only below a specific ID are required set max_id to that ID. Otherwise default to no upper limit and start from the most recent tweet matching the search query\n",
    "max_id = -1\n",
    "\n",
    "#I did not end up using these filters, but am including them in case they are of use to others:\n",
    "#A filter to only download tweets from verified accounts.\n",
    "#Can be useful if you want to download tweets on a specific topic as opposed to from a specific person\n",
    "#verified_filter='+filter:verified'\n",
    "#A filter to only download tweets in English.\n",
    "#Note that this sometimes fails, e.g. tweets can have an Ensligh URL with non-English words\n",
    "#In that case you can employ a function to filter out all the non-English tweets later in the pipleline\n",
    "#lang_filter = '+language = \"en\"'\n",
    "#A filter to only download tweets that are not simple re-tweets.\n",
    "#rt_filter = '-filter:retweets'\n",
    "\n",
    "tweetCount = 0\n",
    "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "with open(fName, 'w') as f:\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    #You can exclude retweets either with a filter or by directly specifying that include_rts=False\n",
    "                    new_tweets = api.user_timeline(screen_name = account_list1, include_rts = False, count=tweetsPerQry)\n",
    "                else:\n",
    "                    new_tweets = api.user_timeline(screen_name = account_list1, include_rts = False, count=tweetsPerQry,\n",
    "                                            since_id=sinceId)\n",
    "            else:\n",
    "                if (not sinceId):\n",
    "                    new_tweets = api.user_timeline(screen_name = account_list1, include_rts = False, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1))\n",
    "                else:\n",
    "                    new_tweets = api.user_timeline(screen_name = account_list1, include_rts = False, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1),\n",
    "                                            since_id=sinceId)\n",
    "            if not new_tweets:\n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "            for tweet in new_tweets:\n",
    "                results1.append(tweet)\n",
    "#                f.write(jsonpickle.encode(tweet._json, unpicklable=False) +\n",
    "#                       '\\n')\n",
    "            tweetCount += len(new_tweets)\n",
    "            print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "            max_id = new_tweets[-1].id\n",
    "        except tweepy.TweepError as e:\n",
    "            # Just exit if any error\n",
    "            print(\"some error : \" + str(e))\n",
    "            break\n",
    "\n",
    "print (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))\n",
    "\n",
    "\n",
    "#takes the list results1 and returns a dataframe after storing results in it\n",
    "def process_results1(results1):\n",
    "    id_list = [tweet.id for tweet in results1]\n",
    "    df_name = pd.DataFrame(id_list, columns = [\"id\"])\n",
    "\n",
    "#Processing the tweet data by putting it all in a dataframe\n",
    "    df_name[\"text\"] = [tweet.text for tweet in results1]\n",
    "    df_name[\"created_at\"] = [tweet.created_at for tweet in results1]\n",
    "    df_name[\"retweet_count\"] = [tweet.retweet_count for tweet in results1]\n",
    "    df_name[\"favorite_count\"] = [tweet.favorite_count for tweet in results1]\n",
    "    df_name[\"source\"] = [tweet.source for tweet in results1]\n",
    "#processing the user data\n",
    "    df_name[\"user_id\"] = [tweet.author.id for tweet in results1]\n",
    "    df_name[\"user_screen_name\"] = [tweet.author.screen_name for tweet in results1]\n",
    "    df_name[\"user_name\"] = [tweet.author.name for tweet in results1]\n",
    "#when the user created the account:\n",
    "    df_name[\"user_created_at\"] = [tweet.author.created_at for tweet in results1]\n",
    "    df_name[\"user_description\"] = [tweet.author.description for tweet in results1]\n",
    "    df_name[\"user_followers_count\"] = [tweet.author.followers_count for tweet in results1]\n",
    "    df_name[\"user_friends_count\"] = [tweet.author.friends_count for tweet in results1]\n",
    "    df_name[\"user_location\"] = [tweet.author.location for tweet in results1]\n",
    "\n",
    "#I chose to keep only the last 3000 tweets\n",
    "    return df_name\n",
    "df_name = process_results1(results1)\n",
    "df_name = df_name.iloc[df_name.index < 3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping only the column that has the text in it\n",
    "df_name = df_name.loc[:,['text']]\n",
    "#Splitting the tweets into three columns, and then combining them to create data points of 3 tweets instead of one\n",
    "#I ended up using units of 9 tweets to improve the F1 score\n",
    "df_name = (pd.DataFrame(df_name.values.reshape(-1, 3)))\n",
    "df_name['text'] = df_name[0] + ' ' + df_name[1] + ' ' + df_name[2]\n",
    "df_name = df_name.loc[:,['text']]\n",
    "#Since this person was depressed I added a column to indicate that of their tweets should be classified as such\n",
    "df_name['depressed'] = 1\n",
    "df_name\n",
    "\n",
    "\n",
    "#Repeat this process until you have all the tweets from the different people that you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you find you are having issues playing with the dataframe because of empty cells, try the following\n",
    "#to strip the space from the otherwise empty tweet cells\n",
    "df_name['text']=df_name['text'].astype(\"str\")\n",
    "df_name['text']=df_name['text'].map(str.strip)\n",
    "#creating a filter that is selecting the not empty cells since the space has been stripped\n",
    "filter = df_name[\"text\"] != \"\"\n",
    "#removing all the rows that have cells that are empty that aren't what the filter holds\n",
    "df_name = df_name[filter]\n",
    "#Resetting the index\n",
    "df_name = df_name.reset_index(drop=True)\n",
    "df_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once you have all your data in seperate dataframes combine them:\n",
    "#creating single df\n",
    "#specifying that axis=0 will join the dataframes by coluns--one on top of the other--as opposed to by rows--one next to the other\n",
    "data_set = pd.concat([df_name1,df_name2,df_name3,df_name4], axis=0)\n",
    "data_set = data_set.reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
