{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To avoid data leakage I needed to avoid the traditional 80/20 split for ML validation\n",
    "#Earlier on I created 2 dataframes (data_set for training and data_set_test) so that I could be sure that I had different tweeters in my training and testing sets\n",
    "#This is why I need to run all the processing on two dataframes\n",
    "#If you were going to go the 80/20 route, this is how you would do it:\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#train, test = train_test_split(data_set_bayes, test_size=0.2\n",
    "#It automatically randomly selects 20% of the data\n",
    "\n",
    "#Preparing the testing data:\n",
    "\n",
    "#keeping only the relevant columns\n",
    "data_set_np_test = data_set_test.loc[:,['text', 'depressed']]\n",
    "#stripping the space from the otherwise empty tweet cells\n",
    "data_set_np_test['text']=data_set_np_test['text'].astype(\"str\")\n",
    "data_set_np_test['text']=data_set_np_test['text'].map(str.strip)\n",
    "#creating a filter that is selecting the not empty cells since the space has been stripped\n",
    "filter = data_set_np_test[\"text\"] != \"\"\n",
    "#removing all the rows that have cells that are empty that aren't what the filter holds\n",
    "data_set_np_test = data_set_np_test[filter]\n",
    "data_set_np_test['text'] = [''.join([(re.sub('[^A-Za-z]', ' ', line)) for line in lists]).strip() for lists in data_set_np_test['text']]  \n",
    "#Removing URLS and @ and twitter handles \n",
    "data_set_np_test['text'] = data_set_np_test['text'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))    \n",
    "data_set_np_test['text'] = data_set_np_test['text'].apply(lambda x: re.sub(r\"@\\S+\", \"\", x))\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "data_set_np_test['text'] = data_set_np_test['text'].apply(lambda x: re.sub('  ', ' ', x.lower()))\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#stemming the text\n",
    "porter = PorterStemmer()\n",
    "data_set_np_test['text'] = [porter.stem(word) for word in data_set_np_test['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure that everything looks good:\n",
    "data_set_np_test['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the training data:\n",
    "\n",
    "data_set_bayes = data_set.loc[:,['depressed', 'text']]\n",
    "#Removing URLS and @ and twitter handles \n",
    "data_set_bayes['text'] = data_set_bayes['text'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))    \n",
    "data_set_bayes['text'] = data_set_bayes['text'].apply(lambda x: re.sub(r\"@\\S+\", \"\", x))\n",
    "\n",
    "data_set_bayes['text'] = data_set_bayes['text'].apply(lambda x: re.sub('  ', ' ', x.lower()))\n",
    "\n",
    "porter = PorterStemmer()\n",
    "data_set_bayes['text'] = [porter.stem(word) for word in data_set_bayes['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking that everything looks fine:\n",
    "data_set_bayes['text'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following, in effect, is curbing data leakage.\n",
    "#Because I had so few people providing all of the tweets the algorithim was picking up on how the tweets of the people in the \n",
    "#categories differed, as opposed to how the categories were the same within each other and one category differed from the other\n",
    "#That is, it was using the person as the feature\n",
    "#through trial and error I realized that I can mitigate the impact by extracting the important features, \n",
    "#and just deleting from all the tweets the features that were probably unique to only a single person (because of how who they were differed from everyone else)\n",
    "#I also didn't want to remove all the stop words, but realized that I did want to remove some of them\n",
    "#These are most of the features that I removed:\n",
    "\n",
    "train = data_set_bayes\n",
    "test = data_set_np_test\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"homeschool\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"homeschool\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"kid\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"kid\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"kids\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"kids\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"lily\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"lily\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"lilly\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"lilly\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"facebook\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"facebook\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"marketing\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"marketing\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"linkedin\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"linkedin\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"dad\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"dad\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"lebron\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"lebron\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"buzzie\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"buzzie\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"voiceover\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"voiceover\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"stream\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"stream\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"george\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"george\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"xxx\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"xxx\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"the\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"the\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"it\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"it\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"is\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"is\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"in\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"in\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"that\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"that\", \"\", x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r\"than\", \"\", x))    \n",
    "test['text'] = test['text'].apply(lambda x: re.sub(r\"than\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest didn't work for me, but here is the code that I experimented with:\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Verbose prints out information about progress of tree building\n",
    "text_clf_rf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf', RandomForestRegressor(n_jobs=-1, verbose=2))])\n",
    "\n",
    "text_clf_rf = text_clf_rf.fit(train.text, train.depressed)\n",
    "\n",
    "# Performance of RF Classifier\n",
    "import numpy as np\n",
    "predicted = text_clf_rf.predict(test.text)\n",
    "np.mean(predicted == test.depressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examining a Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Although it is possible to integrate these 3 lines into the pipeline so that you only have a single line of code\n",
    "#I have found that in order to later extract features of interest from each step I have to do it this way\n",
    "vect = CountVectorizer(max_features=5500, ngram_range=(1,1))\n",
    "clf = MultinomialNB()\n",
    "tfidf = TfidfTransformer(use_idf=False)\n",
    "\n",
    "text_clf = Pipeline([('vect', vect), ('tfidf', tfidf), ('clf', clf)])\n",
    "\n",
    "text_clf = text_clf.fit(train.text, train.depressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting and saving a confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(test.depressed, predicted)\n",
    "confusion_matrix\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "#cmap='binary' switch to make it BW\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "sn.heatmap(confusion_matrix, annot=True,annot_kws={\"size\": 16},fmt='g', cmap='coolwarm')# font size)\n",
    "plt.savefig('confusion_matrix.png', bbox_inches='tight')#to avoid white borders\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model:\n",
    "import pickle\n",
    "filename_nb_depression_model = '/home/nb_depression_model.sav'\n",
    "pickle.dump(text_clf, open(filename_nb_depression_model, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model: \n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "filename_nb_depression_model = '/home/nb_depression_model.sav'\n",
    "nb_clf = pickle.load(open(filename_nb_depression_model, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To have the model predict new test put it in the Xnew\n",
    "Xnew = ['Put Text You Want Analyzed Here']\n",
    "        # make a prediction\n",
    "ynew = nb_clf.predict_proba(Xnew)\n",
    "# show the inputs and predicted probabilities\n",
    "for i in range(len(Xnew)):\n",
    "    print(\"Percentage match to depressed people's tweets=%s\" % ((ynew[0][1])*100))\n",
    "    print(\"Percentage match to non-depressed people's tweets=%s\" % ((ynew[0][0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "vect = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "#If you want to run a regular regression instead of a CV just erase those two letters\n",
    "clf = LogisticRegressionCV(max_iter=100)\n",
    "tfidf = TfidfTransformer(use_idf=True)\n",
    "\n",
    "text_clf_lr = Pipeline([('vect', vect), ('tfidf', tfidf), ('clf', clf)])\n",
    "#text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "text_clf_lr = text_clf_lr.fit(train.text, train.depressed)\n",
    "\n",
    "import numpy as np\n",
    "predicted = text_clf_lr.predict(test1.text)\n",
    "np.mean(predicted == test1.depressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix for the LR model:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(test.depressed, predicted)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the confusion matrix as a heatmap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(test.depressed, predicted)\n",
    "confusion_matrix\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "#cmap='binary' switch to make it BW\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "sn.heatmap(confusion_matrix, annot=True,annot_kws={\"size\": 16},fmt='g', cmap='coolwarm')# font size)\n",
    "plt.savefig('lr_confusion_matrix.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the top 20 most influencial features (aka words) from the model:\n",
    "def show_most_informative_features(vect, clf, n=20):\n",
    "    feature_names = vect.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "        \n",
    "show_most_informative_features(vect, clf, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(text_clf_lr, 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model and loading it:\n",
    "lr_clf = pickle.dumps(text_clf_lr)\n",
    "lr_clf = pickle.loads(lr_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to save and load the model:\n",
    "import pickle\n",
    "=filename_lr_depression_model = 'lr_depression_model.sav'\n",
    "pickle.dump(text_clf_lr, open(filename_lr_depression_model, 'wb'))\n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_clf = pickle.load(open(filename_lr_depression_model, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To have the model predict new test put it in the Xnew\n",
    "Xnew = ['Put Text You Want Analyzed Here']\n",
    "        # make a prediction\n",
    "ynew = lr_clf.predict_proba(Xnew)\n",
    "# show the inputs and predicted probabilities\n",
    "for i in range(len(Xnew)):\n",
    "    print(\"Percentage match to depressed people's tweets=%s\" % ((ynew[0][1])*100))\n",
    "    print(\"Percentage match to non-depressed people's tweets=%s\" % ((ynew[0][0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM model\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = CountVectorizer(max_features=5000, ngram_range=(1, 1))\n",
    "#because there are only 2 outcomes loss needs to be set to log\n",
    "clf = SGDClassifier(loss='log', penalty='l2', alpha=0.0009, n_iter=35, random_state=42)\n",
    "tfidf = TfidfTransformer(use_idf=False)\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', vect), ('tfidf', tfidf), ('clf', clf)])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(train.text, train.depressed)\n",
    "predicted_svm = text_clf_svm.predict(test.text)\n",
    "np.mean(predicted_svm == test.depressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM performance metrics\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(test.depressed, predicted_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphing and calculating ROC curve\n",
    "fpr, tpr, threshold = metrics.roc_curve(test.depressed, predicted_svm)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What the dictionary keys for the top 5 words in the dictionary look like\n",
    "#Play with max_features to visualize more words\n",
    "#play with the ngrams to look not only at single words but also/or groups of 2 or 3 words\n",
    "vect = CountVectorizer(max_features=5, ngram_range=(1, 1))\n",
    "vect.fit(train.text)\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take those 5 words and make a dictionary arry out of them, and visualize what the matrix looks like\n",
    "vector = vect.transform(train.text)\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see what happens when you apply TF to the matrix\n",
    "#To check out IDF as well set use_idf to true\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(vector)\n",
    "print(X_train_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_svm.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the top 65 features of the models:\n",
    "feature_names = vect.get_feature_names()\n",
    "coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "#20 is the number of features we want extracted\n",
    "top = zip(coefs_with_fns[:65], coefs_with_fns[:-(65 + 1):-1])\n",
    "for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(test.depressed, predicted_svm)\n",
    "confusion_matrix\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "#cmap='binary' switch to make it BW\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "sn.heatmap(confusion_matrix, annot=True,annot_kws={\"size\": 16},fmt='g', cmap='coolwarm')# font size)\n",
    "plt.savefig('lr_confusion_matrix.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search wasn't appropriate for me because of my data leakage problem, \n",
    "#But it's usually a good way to figure out how to tune the hyperparameters\n",
    "#I am extracting both how to tune the parameters\n",
    "#As well as what the highest validation score that I can expect is\n",
    "\n",
    "#fine tuning NB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#creating a list of parameters for which we would like to do performance tuning. \n",
    "#All the parameters name start with the classifier name \n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3), (2, 2), (3,3)],\n",
    "               'tfidf__use_idf': (True, False),\n",
    "               'clf__alpha': (1e-2, 1e-3),}\n",
    "#we are telling to use unigram and bigrams and choose the one which is optimal\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(train.text, train.depressed)\n",
    "#to see the best params\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to see the best mean score\n",
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tuning lr\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_lr = {'vect__ngram_range': [(1, 1), (1, 2), (2, 2), (1, 3), (2, 3), (3, 3)],\n",
    "               'tfidf__use_idf': (True, False),}\n",
    "gs_clf_lr = GridSearchCV(text_clf_lr, parameters_lr, n_jobs=-1)\n",
    "gs_clf_lr = gs_clf_lr.fit(train.text, train.depressed)\n",
    "gs_clf_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to see the best mean score\n",
    "gs_clf_lr.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tuning SVM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'tfidf__use_idf': (True, False),\n",
    "               'clf-svm__alpha': (1e-2, 1e-3, 1e-4, 1e-5, 1e-6),}\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(train.text, train.depressed)\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_svm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The neural network was probably overfitting badly because of the data leakage but it was fun to play with\n",
    "#Checking that TensorFlow works\n",
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))\n",
    "#Should print out Hellow TensorFlow if it is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_set_np= data_set.loc[:,['text', 'depressed']]\n",
    "training = data_set_np.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our training data from the tweets\n",
    "train_x = [x[0] for x in training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index all the tweets\n",
    "train_y = np.asarray([x[1] for x in training])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only work with the 7000 most popular words found in our dataset\n",
    "max_words = 7000\n",
    "# create a new Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# feed our tweets to the Tokenizer\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "# Let's save this out so we can use it later\n",
    "with open('dictionary.json', 'w') as dictionary_file_nn:\n",
    "    json.dump(dictionary, dictionary_file_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_index_array(text):\n",
    "    # one really important thing that `text_to_word_sequence` does\n",
    "    # is make all texts the same length -- in this case, the length\n",
    "    # of the longest text in the set.\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\n",
    "\n",
    "allWordIndices = []\n",
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have a list of all tweets converted to index arrays.\n",
    "# cast as an array for future usage.\n",
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "# create one-hot matrices out of the indexed tweets\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "# treat the labels as categories\n",
    "train_y = keras.utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras’ Sequential() is a simple type of neural net that consists of a “stack” of layers executed in order\n",
    "#The input and output layers are the most important, since they determine the overall shape of the neural net. \n",
    "#Out network will mostly consist of Dense layers — the “standard”, linear neural net layer of inputs, weights, and outputs.\n",
    "#In our case, we’re inputting a sentence that will be converted to a one-hot matrix of length max_words — here, 3000. \n",
    "#We also include how many outputs we want to come out of that layer (512, for funsies) \n",
    "#and what kind of maximization (or “activation”) function to use.\n",
    "#Activation functions are used when training the network; \n",
    "#they tell the network how to judge when a weight for a particular node has created a good fit.\n",
    "# Activation functions differ, mostly in speed, but all the ones available in Keras and TensorFlow are viable; feel free to play around with them. \n",
    "#If you don’t explicitly add an activation function, that layer will use a linear one.\n",
    "#Our output layer consists of two possible outputs, since that’s how many categories our data could get sorted into. \n",
    "#If you use a neural net to predict rather than classify, you’re actually creating a neural net with one possible output — the prediction.\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_dim is set to the max words\n",
    "model.add(Dense(512, activation='relu', input_dim=7000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropouts are used to randomly remove data, which can help avoid overfitting.\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(256, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the network:\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting (training) our model off of inputs train_x and categories train_y\n",
    "history = model.fit(train_x, train_y,\n",
    "#We evaluate data in groups of batch_size, checking the network’s accuracy, tweaking node weights, and then running through another batch. \n",
    "#Small batches let you train networks much more quickly than if you tried to use a batch the size of your entire training dataset.\n",
    "  batch_size=128,\n",
    "#epochs is how many times you do this batch-by-batch splitting. I’ve found 5 to be good in this case; I tried 7, but ended up overfitting\n",
    "  epochs=5,\n",
    "  verbose=1,\n",
    "#validation_split says how much of your input you want to be reserved for testing data\n",
    "#essential for seeing how accurate your network is at that point. \n",
    "#Recommended training-to-test ratios are 80:20 or 90:10.\n",
    "  validation_split=0.1,\n",
    "  shuffle=True)\n",
    "#The value to watch is not acc but val_acc, or Validation Accuracy. \n",
    "#This is your neural net's score when predicting values for data in your validation split\n",
    "#Your accuracy should start out low per epoch and rise throughout the epoch; it should increase at least a little across epochs. \n",
    "#If your accuracy starts decreasing, you’re overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model so that you don’t have to keep repeating all of those steps\n",
    "#Your model gets saved in two parts: One is the model’s structure itself\n",
    "model_json_nn = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json_nn)\n",
    "#other is the weights used in those model’s nodes.\n",
    "model.save_weights('model_nn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're still going to use a Tokenizer here, but we don't need to fit it\n",
    "tokenizer = Tokenizer(num_words=7000)\n",
    "# for human-friendly printing\n",
    "labels = ['not depressed', 'depressed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in our saved dictionary\n",
    "with open('dictionary.json', 'r') as dictionary_file_nn:\n",
    "    dictionary = json.load(dictionary_file_nn)\n",
    "\n",
    "# this utility makes sure that all the words in your input\n",
    "# are registered in the dictionary\n",
    "# before trying to turn them into a matrix.\n",
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "        else:\n",
    "            print(\"'%s' not in training corpus; ignoring.\" %(word))\n",
    "    return wordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in your saved model structure\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# and create a model from that\n",
    "model = model_from_json(loaded_model_json)\n",
    "# and weight your nodes with your saved values\n",
    "model.load_weights('model_nn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay here's the interactive part\n",
    "while 1:\n",
    "    evalSentence = input('Input a sentence to be evaluated, or Enter to quit: ')\n",
    "\n",
    "    if len(evalSentence) == 0:\n",
    "        break\n",
    "\n",
    "    # format your input for the neural net\n",
    "    testArr = convert_text_to_index_array(evalSentence)\n",
    "    inputed_text = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "    # predict which bucket your input belongs in\n",
    "    pred = model.predict(inputed_text)\n",
    "    # and print it for the humons\n",
    "    print(\"%s sentiment; %f%% confidence\" % (labels[np.argmax(pred)], pred[0][np.argmax(pred)] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you have issues running the above section more than once try:\n",
    "reset_selective inputed_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
